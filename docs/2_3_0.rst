Identify the Best Model
*************

.. article-info::
    :avatar: dnl_plastic.png
    :avatar-link: https://www.decisionneurosciencelab.com/
    :author: Elijah Galvan
    :date: September 1, 2023
    :read-time: 10 min read
    :class-container: sd-p-2 sd-outline-muted sd-rounded-1

Lesson
================

Goal During this Stage
---------------

To determine which model is best. 

.. Note::

    Now if you've been following along, you've probably realized that we've only talked so far about a single model - I'll briefly explain why. 
    We use Social Utility Models to encapsulate individual differences in value based preferences and exisiting research on this topic essentially shows that **people are usually different in most conceivable ways**. 
    Thus, we always try to design experiments around trying to see this individual differences in behavior and our "favored" model - the one we think will best fit the data - is the one which captures all of these differences.
    Since the favored model must capture all meaningful potential differences, it is therefore the most complex model that we will test.

    So why are we reiterating this now? 
    Well, the answer is that any *other types* of models you test on value-based decision-making behavior in non-forced, non-probabilistic, social choice tasks are almost always going to perform much worse. 
    These tasks are designed to elicit consistent, well-defined preferences with a very high signal to noise ratio: therefore, models capturing other aspects of the decision-making process other than these preferences are theoretically and pragmatically inferior in these scenarios. 
    All of this to say, we are going to test this favored model - which is the most complex model - against models which are simplifications of this model.
    Thus, we can use the same :bdg-secondary:`Construct Value` computations, removing one of these :bdg-secondary:`Construct Value` computations from the utility equation and a :bdg-success:`Free Parameter` per model.

How to Achieve this Goal
------------

.. dropdown:: Create :bdg-warning:`Utility` Equations for our Alternative Models

    Generally, you should first enumerate all of the reasonable potential alternative models. 
    This means that if you have a model with 3 different :bdg-secondary:`Construct Value` terms (as most models that we will talk about indeed do) then the alternative model set would include models with only 2 or 1 :bdg-secondary:`Construct Value` terms.
    Are any models not worth testing? 
    Then don't include them. 
    For every model we think is worth testing, let's talk about how you're going to go about testing them.

    Any models that we want to test with one :bdg-secondary:`Construct Value` term are really, really simple - :bdg-warning:`Utility` = :bdg-secondary:`Construct Value`. 
    Notice that there are no :bdg-success:`Free Parameters` in this Equation - since only one :bdg-secondary:`Construct Value` determines :bdg-warning:`Utility`, there is no way that it can capture individual differences. 

    The models that we want to test with two :bdg-secondary:`Construct Value` terms are also pretty simple - we want to capture how people prioritize one norm over another. 
    Thus, we express this as :bdg-warning:`Utility` = (:bdg-secondary:`Construct Value 1` x :bdg-success:`Free Parameter`) + (:bdg-secondary:`Construct Value 2` x (1 - :bdg-success:`Free Parameter`)).

.. dropdown:: Preallocating and Defining Functions 

    .. tab-set::

        .. tab-item:: Plain English

            We're going to need several items for this section:

            1. :bdg-warning:`Utility` Functions for each each Model
            2. Objective Functions for each :bdg-warning:`Utility` Equation with :bdg-success:`Free Parameters`
            3. New Optimizer Inputs for each :bdg-warning:`Utility` Equation with :bdg-success:`Free Parameters`
            4. Outputs for the Alternative Models
        
        .. tab-item:: R

            :: 

                utility_alt1 = function(construct1){
                    return(utility)
                }
                utility_alt2 = function(construct1, construct2, parameter1){
                    return(utility)
                }

                #alternative model with 1 construct has 0 free parameters: doesn't need an objective function

                obj_function_alt2 = function(param, decisions, method = "OLS") {
                    Parameter1 = param[1]
                    
                    predicted_utility = vector('numeric', length(trialList[,1]))
                    chosen = decisions + 1
                    for (k in 1:length(trialList[,1])){
                        IV = trialList[k, 1]
                        Constant = trialList[k, 2]
                        Choices = seq(0, (I * M), 1)
                        
                        Utility = vector('numeric', length(Choices))
                        for (n in 1:length(Choices)){
                        Utility[n] = utility_alt2(Parameter1, construct1(IV, Constant, Choices[n]))
                        }
                        predicted_utility[k] = max(Utility)
                        observed_utility[k] = Utility[chosen[k]]
                    }
                    if (method == "OLS"){
                        return(sum((predicted_utility - observed_utility)**2))
                    } else if (method == "MLE"){
                        return(-1 * sum(dnorm(observed_utility, mean = predicted_utility, sd = sd, log = TRUE)))
                    }
                }

                initial_param_alt2 = #something
                lower_bound_alt2 = #something
                upper_bound_alt2 = #something

                altSubjectData = data.frame()

        .. tab-item:: MatLab

        .. tab-item:: Python



.. dropdown:: Recover :bdg-success:`Free Parameters` for Each Alternative Model, Per :bdg-success:`Subject`

    .. tab-set::

        .. tab-item:: Plain English

           For the alternative models with :bdg-success:`Free Parameters`, we'll need to recover these :bdg-success:`Free Parameters` in order to generate model predictions. 
           Let's do that quickly in the same way that we did for the other model, leaving a demand to subsequently determine model predictions.
           
           .. Note::
            
                Models with only one :bdg-success:`Free Parameter` may require a different optimzer than models with multiple :bdg-success:`Free Parameters`. 
                However, you won't have to change anything about how your objective function works or anything like that so don't worry!
        
        .. tab-item:: R

            :: 

                for (i in 1:length(included_subjects)){
                    datafile = paste(parentfolder, included_subjects[i], restoffilepath, sep = '') # produces a character vector 'parentfolder/included_subjects[i]**.filetype'
                    df = read.csv2(datafile)
                    reorder = df$trialsTask.thisIndex + 1

                    result_alt2 = optim(obj_function_alt2, par = initial_param_alt2, lower = lower_bound_alt2, upper = upper_bound_alt2, decisions = df$Decisions)

                    # Determine Predictions
                }

        .. tab-item:: MatLab

        .. tab-item:: Python

.. dropdown:: Determine Predicted :bdg-danger:`Decisions` for Each Alternative Model, Per :bdg-success:`Subject`

    .. tab-set::

            .. tab-item:: Plain English

                Now, we are going to answer the Determine Predictions demand placed on us.
                We have found the :bdg-success:`Subject`'s :bdg-success:`Free Parameters` so we need to specifically know what it is that our model predicts that they will do.
                In the previous step, we could have cut a corner and gotten the predictions from the closest point we simulated data for. 
                In all likelihood, the model predictions would be indistinguishable from these, but for the sake of being punctual let's get these predictions in the same way we did with our favored model!

            .. tab-item:: R

                ::

                    for (i in 1:length(included_subjects)){
                        datafile = paste(parentfolder, included_subjects[i], restoffilepath, sep = '') # produces a character vector 'parentfolder/included_subjects[i]**.filetype'
                        df = read.csv2(datafile)
                        reorder = df$trialsTask.thisIndex + 1

                        result_alt2 = optim(obj_function_alt2, par = initial_param_alt2, lower = lower_bound_alt2, upper = upper_bound_alt2, decisions = df$Decisions)

                        #Just Added
                        
                        df$PredictionAlt1 = vector('numeric')
                        df$PredictionAlt2 = df$PredictionAlt1
                        for (k in 1:length(df$Decisions)){
                            UtilityAlt1 = vector('numeric', length(Choices))
                            UtilityAlt2 = vector('numeric', length(Choices))
                            for (n in 1:length(Choices)){
                                UtilityAlt1[n] = utility_alt1(construct1 = construct1(df$IV[k], df$Constant[k], Choices[n]))
                                UtilityAlt2[n] = utility_alt2(parameter1 = result_alt2$par[1],
                                                              construct1 = construct1(df$IV[k], df$Constant[k], Choices[n]),
                                                              construct2 = construct2(df$IV[k], df$Constant[k], Choices[n]))
                            }
                            correct_choice_alt1 = which(UtilityAlt1 == max(UtilityAlt1))
                            correct_choice_alt2 = which(UtilityAlt2 == max(UtilityAlt2))
                            if (length(correct_choice) > 1){
                                correct_choice = correct_choice[sample(correct_choice, 1)]
                            }
                            df$PredictionAlt1[k] = Choices[correct_choice_alt1]
                            df$PredictionAlt2[k] = Choices[correct_choice_alt2]
                        }

                        model_NLL_Alt1 = -2 * log(sum(dnorm(df$Decision, mean = df$PredictionAlt1)))
                        model_SS_Alt1 = sum((df$Decision - df$PredictionAlt1)**2)
                        model_NLL_Alt2 = -2 * log(sum(dnorm(df$Decision, mean = df$PredictionAlt2)))
                        model_SS_Alt2 = sum((df$Decision - df$PredictionAlt2)**2)

                        altSubjectData[i, ] = c(included_subjects[i], result_alt2$par[1], model_NLL_Alt1, model_SS_Alt1, model_NLL_Alt2, model_SS_Alt2) 
                        #add any additional subject-level variables; if we have a priori clusters, you can include the strategy like we've done here
                    }
                    colnames(subjectData) = c('SubjectID', 'Parameter1_Alt2', 'alt1_modelNLL', 'alt1_modelSS', 'alt2_modelNLL', 'alt2_modelSS')

            .. tab-item:: MatLab

                ::

            .. tab-item:: Python
                
                ::

.. dropdown:: Compute Model Fit Index for Each :bdg-success:`Subject`, for Each Alternative Model

    .. tab-set::

        .. tab-item:: Plain English

           Now that we have the model error - either the sum of squared errors or the negative log likelihood of the real :bdg-success:`Subjects` :bdg-danger:`Decisions` versus the model's predicted :bdg-danger:`Decisions`.
        
        .. tab-item:: R

            :: 

                altSubjectData$modelAlt1AIC = N * log(altSubjectData$modelSS_Alt1/N) + 2*0
                altSubjectData$modelAlt2AIC = N * log(altSubjectData$modelSS_Alt2/N) + 2*1

        .. tab-item:: MatLab

        .. tab-item:: Python

.. dropdown:: Compare Model Performance

    .. tab-set::

        .. tab-item:: Plain English

            Now we simply want to identify which model is best. 
            Thus, we're going to create a vector with the MFI for each model averaged across all :bdg-success:`Subjects` and select the model with the lowest MFI. 
            This approach tells us which model provides the best average fit for :bdg-success:`Subjects` in our sample. 
            Importantly, if any :bdg-success:`Subjects` data are fully explained by the model (i.e. observed :bdg-danger:`Decisions` always equal :bdg-danger:`Decisions` predicted by the model) then these :bdg-success:`Subjects` must be excluded from your analysis since their MFIs are negative infinity. 

            Another approach would be to compute MFIs for the entire dataset - this approach does not require that you exclude :bdg-success:`Subjects` from analysis. 
            It is more appropriate to use the first approach if you are focused on individual differences (i.e. trying to characterize how people are different) rather than general trends in behavior (i.e. tring to characterize how various factors affect decision-making within a person).
        
        .. tab-item:: R

            :: 

                excluded = which(is.inf(subjectData$modelAIC) | is.inf(altSubjectData$modelAlt1AIC) | is.inf(altSubjectData$modelAlt2AIC))
                averageAIC = (mean(subjectData$modelAIC[-excluded]), mean(altSubjectData$modelAlt1AIC[-excluded]), mean(altSubjectData$modelAlt2AIC[-excluded]))
                fullAIC = length(trialData$SubjectID) * log(sum(subjectData$modelSS)/length(trialData$SubjectID)) + (2 * k * length(subjectData$SubjectID))
                fullAICAlt1 = length(trialData$SubjectID) * log(sum(altSubjectData$modelAlt1SS)/length(trialData$SubjectID)) + (2 * 0 * length(subjectData$SubjectID))
                fullAICAlt2 = length(trialData$SubjectID) * log(sum(altSubjectData$modelAlt2SS)/length(trialData$SubjectID)) + (2 * 0 * length(subjectData$SubjectID))

                bestModel = c("Favored Model", "Alternative Model 1", "Alternative Model 2")[which(averageAIC == min(averageAIC))] #best model based on average performance per subject
                bestModelFullDataset = c("Favored Model", "Alternative Model 1", "Alternative Model 2")[which(c(fullAIC, fullAICAlt1, fullAICAlt2) == min(c(fullAIC, fullAICAlt1, fullAICAlt2)))] #best model based on all data observations

        .. tab-item:: MatLab

            ::

                excluded = find(isinf(subjectData.modelAIC) | isinf(altSubjectData.modelAlt1AIC) | isinf(altSubjectData.modelAlt2AIC));
                averageAIC = [mean(subjectData.modelAIC(~excluded)), mean(altSubjectData.modelAlt1AIC(~excluded)), mean(altSubjectData.modelAlt2AIC(~excluded))];
                fullAIC = length(trialData.SubjectID) * log(sum(subjectData.modelSS)/length(trialData.SubjectID)) + (2 * k * length(subjectData.SubjectID));
                fullAICAlt1 = length(trialData.SubjectID) * log(sum(altSubjectData.modelAlt1SS)/length(trialData.SubjectID)) + (2 * 0 * length(subjectData.SubjectID));
                fullAICAlt2 = length(trialData.SubjectID) * log(sum(altSubjectData.modelAlt2SS)/length(trialData.SubjectID)) + (2 * 0 * length(subjectData.SubjectID));

                [~, bestModel] = min(averageAIC); % best model based on average performance per subject
                [~, bestModelFullDataset] = min([fullAIC, fullAICAlt1, fullAICAlt2]); % best model based on all data observations
                bestModel = {'Favored Model', 'Alternative Model 1', 'Alternative Model 2'}{bestModel};
                bestModelFullDataset = {'Favored Model', 'Alternative Model 1', 'Alternative Model 2'}{bestModelFullDataset};


        .. tab-item:: Python

            ::
                
                excluded = np.where(np.isinf(subjectData['modelAIC']) | np.isinf(altSubjectData['modelAlt1AIC']) | np.isinf(altSubjectData['modelAlt2AIC']))[0]
                averageAIC = [np.mean(subjectData['modelAIC'][~excluded]), np.mean(altSubjectData['modelAlt1AIC'][~excluded]), np.mean(altSubjectData['modelAlt2AIC'][~excluded])]
                fullAIC = len(trialData['SubjectID']) * np.log(np.sum(subjectData['modelSS']) / len(trialData['SubjectID'])) + (2 * k * len(subjectData['SubjectID']))
                fullAICAlt1 = len(trialData['SubjectID']) * np.log(np.sum(altSubjectData['modelAlt1SS']) / len(trialData['SubjectID'])) + (2 * 0 * len(subjectData['SubjectID']))
                fullAICAlt2 = len(trialData['SubjectID']) * np.log(np.sum(altSubjectData['modelAlt2SS']) / len(trialData['SubjectID'])) + (2 * 0 * len(subjectData['SubjectID']))

                bestModel = ['Favored Model', 'Alternative Model 1', 'Alternative Model 2'][np.argmin(averageAIC)]  # best model based on average performance per subject
                bestModelFullDataset = ['Favored Model', 'Alternative Model 1', 'Alternative Model 2'][np.argmin([fullAIC, fullAICAlt1, fullAICAlt2])]  # best model based on all data observations


Tutorials
==========