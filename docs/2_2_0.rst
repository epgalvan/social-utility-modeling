Compute Model Fit Index
*************

.. article-info::
    :avatar: dnl_plastic.png
    :avatar-link: https://www.decisionneurosciencelab.com/
    :author: Elijah Galvan
    :date: September 1, 2023
    :read-time: 10 min read
    :class-container: sd-p-2 sd-outline-muted sd-rounded-1

Lesson
================

Goals During this Stage
---------------

To compare models in terms of how well they predict :bdg-danger:`Decisions` made by each :bdg-success:`Subjects`, accounting for how parsimonious each model is. 

How to Achieve this Goal
---------------

.. dropdown:: Select a Model Fit Index

    There are a few options that you have in terms of chosing between existing Model Fit Indices (MFIs). 
    The standard choices are between the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). 
    Often, these MFIs will produce the same conclusions though sometimes they do not.
    Therefore, it is important to preregister which MFI you will use to select a model and to test your hypothesis.

    While BIC is often the default MFI, both AIC and BIC have their use cases. 
    Specifically, you should use the BIC if you are convinced that you are modeling the true data generation process. 
    The BIC is superior to the AIC at detecting the model which created data. 
    On the other hand, AIC is favorable if you believe that none of the models that you propose are the true data generation process.
    AIC is superior at identifying the best approximation of the data generation process.
    Therefore, you should always use AIC for generalized models (models without :bdg-success:`Free Parameters` characterizing the actual Decision-Making Process such as noise or bias :bdg-success:`Free Parameters` or without non-experimental variables such as self-report measures)

.. dropdown:: Compute Model Fit Index for Each :bdg-success:`Subject`

    .. tab-set::

        .. tab-item:: Plain English

            AIC is calculated as ``NLL + 2k`` while BIC is calculated as ``NLL + ln(N) * k``
            For both NLL is -2 times the log likelihood of our :bdg-success:`Subject`'s data (we calculated this already), k is the number of Free Parameters in our data, and N is the number of observations (i.e. trials).
            
            Alternatively, if model errors are normally distributed, NLL can be replaced with ``N * ln(SS/N)`` where SS is the sum of squared errors between :bdg-danger:`Decisions` predicted by your model and those actually observed. 
            If you are using a generalized model, your conclusions rely on this assumption. 
            This is because if you are *choosing* to overlook aspects of the data generation process by not modeling noise or biases, you must create a design wherein these tendencies produce random, rather than systematic, error. 
            Otherwise, you might very well end up fitting this systematic error which means that your conclusions are completely invalid.

            We've been very focused on making our models generalizable, so we would use the AIC with the latter formulation but we'll compute the BIC in both formulations for an example here. 
            Remember our model has two free parameters.

        .. tab-item:: R

            ::

                N = length(trialList[, 1])
                k = 2
                subjectData$modelAIC = N * log(subjectData$modelSS/N) + 2*k
                subjectData$modelAICStandard = subjectData$modelNLL + 2*k
                subjectData$modelBIC = N * log(subjectData$modelSS/N) + log(N)*k
                subjectData$modelBICStandard = subjectData$modelNLL + log(N)*k

        .. tab-item:: MatLab

        .. tab-item:: Python

        

Tutorial
==========