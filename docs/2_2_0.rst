Compute Model Fit Index
*************

Goals During this Stage
==========

So now we should have the predicted :bdg-danger:`Decisions` for each :bdg-success:`Subject` which we can compare to the actual :bdg-danger:`Decisions` they made. 
We mentioned already that this will enable us to determine how well the model performed, but how do we actually do this?
We want to assess model accuracy: here, telling us how well our model predicts actual behavior **for each individual subject**. 
In other words, if we determine the prediction error - for instance the sum of squared errors - which will tell us how accurate our model is. 
However, we also want to penalize models for having more and more parameters. 
So how do we mathematically do this?

.. dropdown:: Select a Model Fit Index

    Well, there are a few options that you have in terms of chosing between existing Model Fit Indices (MFIs). 
    The standard choices are between the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). 
    Often, these MFIs will produce the same conclusions though sometimes they do not.
    Therefore, it is important to preregister which MFI you will use to select a model and to test your hypothesis.

    While BIC is often the default MFI, both AIC and BIC have their use cases. 
    Specifically, you should use the BIC if you are convinced that you are modeling the true data generation process. 
    The BIC is superior to the AIC at detecting the model which created data. 
    On the other hand, AIC is favorable if you believe that none of the models that you propose are the true data generation process.
    AIC is superior at identifying the best approximation of the data generation process.
    Therefore, you should use AIC for generalized models (models without :bdg-success:`Free Parameters` characterizing the actual Decision-Making Process such as noise or bias :bdg-success:`Free Parameters`)

.. dropdown:: Compute Model Fit Index for Each :bdg-success:`Subject`

    .. tab-item:: Plain English

        AIC is calculated as ``NLL + 2k`` while BIC is calculated as ``NLL + ln(N) * k``
        For both NLL is -2 times the log likelihood of our :bdg-success:`Subject`'s data (we calculated this already), k is the number of Free Parameters in our data, and N is the number of observations (i.e. trials).
        
        Alternatively, if model errors are normally distributed, NLL can be replaced with ``N * ln(SS/N)`` where SS is the sum of squared errors between model predictions and observed values. 
        If you are using a generalized model, your conclusions rely on this assumption. 
        This is because if you are *choosing* to overlook aspects of the data generation process by not modeling noise or biases, you must create a design wherein these tendencies produce random, rather than systematic, noise. 
        Otherwise, you might very well end up fitting this systematic noise which means that your conclusions might be wrong and are certainly invalid.

        We've been very focused on making our models generalizable, so we would use the AIC with the latter formulation but we'll compute the BIC in both formulations for an example here. 
        Remember our model has two free parameters.

    .. tab-item:: R

        N = length(trialList[, 1])
        k = 2
        subjectData$modelAIC = N * log(subjectData$modelSS/N) + 2*k
        subjectData$modelAICStandard = subjectData$modelNLL + 2*k
        subjectData$modelBIC = N * log(subjectData$modelSS/N) + log(N)*k
        subjectData$modelBICStandard = subjectData$modelNLL + log(N)*k

    .. tab-item:: MatLab

    .. tab-item:: Python

        

Tutorial
==========