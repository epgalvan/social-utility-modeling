Validate the Best Model
*************

Goals During this Stage
==========

In the last stage, we identified the best model in our model set - now we want to see if the model is actually valid. 
Essentially, there are two things that we need to prove in order for our model to be valid: first, we have to show that our model accurately captures the data generation process and, second, we have to show that our parameter recovery process is robust.

In order to show that our model accurately captures the data generation process, we have to show that the model predicts behavior equally well at all values. 
Essentially, we must test four assumptions about how our model predictions relate to the behavioral data that they were trained upon: linearity, normality of error, independence of error, and homoscedasticty. 
These are also four of the five assumptions of linear regression (the fifth is the independence of :bdg-primary:`Independent Variables`, but we don't rely on this assumption).

.. Note:: 

    The assumptions of our model are (with one exception) the assumptions of linear regression, but not the assumptions of mixed effects regression despite the fact that we obviously are using repeated measures - why is this?

    Well, the answer is essentially the same answer as why this fifth assumption - the independence of :bdg-primary:`Independent Variables` - is not included. 
    According to our model, all of the ways that :bdg-success:`Subjects` can be different are encapsulated in the :bdg-success:`Free Parameters`. 
    Another way to phrase the assumption of the independence of :bdg-primary:`Independent Variables` is the independence of observations. 
    Linear modeling requires that observations are not related, so if :bdg-primary:`Independent Variables` are collinear, then the :bdg-danger:`Dependent Variable` are predicted by both :bdg-primary:`Independent Variables` which means that the observations are dependent upon each other. 
    Thus since we don't know what actually causes the :bdg-danger:`Dependent Variable` to change neither :bdg-primary:`Independent Variable` can be used to predict the variance explained by both. 
    In a similar vein, linear mixed effects models control for the independence of observations (i.e. produced by the same :bdg-success:`Subject`) using random effects. 

    See, where we're going here? 
    Since our model asserts that all of the ways that :bdg-success:`Subjects` can be different are encapsulated in the :bdg-success:`Free Parameters`, we essentially assert that the way our model predicts :bdg-danger:`Decisions` does not differ across :bdg-success:`Subjects`. 
    We have one :bdg-primary:`Independent Variable` - the model predicted :bdg-danger:`Decisions`. 
    According to our model, *nothing else* can predict :bdg-danger:`Decisions`. 
    However, we're not going to blindly assume this - we'll also verify that this is true.

.. dropdown:: Model Performance

    .. tab-set::

        .. tab-item:: Plain English

            Before we start checking assumptions, we need to do a simple regression: using the predicted :bdg-danger:`Decisions` of our best model to predict actual :bdg-danger:`Decisions`.
            So, we will have to use the trial-by-trial data to accomplish this.

            What we want to see first is the R-squared: the multiple R-squared should always be essentially equal to the adjusted R-squared since there are going to be a lot of observations. 
            The R-squared tells us how much variance our model explains: any model with an R-squared above 0.70 is very good. 
            Particularly for models without noise parameters, it means that you are really capturing the data generation process and essentially rules out the possibility that you are missing an additionally strategy or additional motive. 
            As long as you don't find any assumptions to be grotesquely violated, you should proceed and feel very confident that your model is a good representation of the data generation process. 

            An R-squared between 0.5 and 0.7 is acceptable. 
            Although this good model performance by objective standards, these tasks are designed to elicit very well-defined, consistent preferences with a very high signal to noise ratio. 
            You might be missing an additional strategy in your model - so use the steps below and see if this is the case!
            If not, you should proceed with apprehension but it does not immediately invalidate any conclusions that you want to draw. 

            An R-squared of less than 0.5 is bad. 
            Either your task-and-experimental procedure was 1) poorly thought out, producing too high of a signal-to-noise ratio, or 2) your model does not account for one or more preferences that govern behavior in your task.
            Ask yourself what the cause is: if it is a problem with the task either consider adding noise or bias parameters, find a different kind of model to analyze your data, or otherwise throw the data out because you cannot analyze it with a utility model. 
            If it is the latter then identify :bdg-success:`Subjects` who are fit particularly poorly by the model, identify the behavioral trend, and try to think about a value-based preference that could lead one to behave in such a way. 
            We can try this on all of these :bdg-success:`Subjects` - if this doesn't offer clear insight then we might want to look at individual subjects.

        .. tab-item:: R

            ::
                
                modelPredictions = lm(data = trialData, Decisions ~ Prediction)
                summary(modelPredictions) # R-squared

                #we can identify if we're missing a strategy using a density plot of MFIs - a plot with a group of really high AICs with a group of lower AICs would suggest a missing strategy
                qplot(x = subjectData$modelAIC, geom = 'density')

                #if we're missing something let's identify worst explained quartile of subjects according to our model            
                worstExplained = which(subjectData$modelAIC > as.numeric(summary(subjectData$modelAIC)[5]))
                qplot(data = trialData[which(trialData$SubjectID == subjectData$SubjectID[worstExplained]), x = IV, y = DV, group = trialData$SubjectID]) + geom_smooth() #a loess line for all subjects

        .. tab-item:: MatLab

        .. tab-item:: Python

.. dropdown:: Visually Checking Assumptions

    .. dropdown:: Linearity

        .. tab-set::

            .. tab-item:: Plain English

                We need to ensure that the relationship between model predictions of :bdg-danger:`Decisions` and observed :bdg-danger:`Decisions` is linear. 
                If this relationship is flat (slope is 0) rather than linear (slope is 1) then our model is doing terribly at predicting :bdg-danger:`Decisions`, essentially making predictions completely at chance-level.
                Thus our model would not be capturing the data generation process and, most likely, our :bdg-success:`Free Parameters` are fitted to noise at all values.
                So let's plot a regression line with model predictions on the x-axis and observed :bdg-danger:`Decisions` on the y-axis. 
                The slope should be essentially 1.

            .. tab-item:: R

                ::
                    
                    qplot(x = trialData$Prediction, y = trialData$Decision, geom = 'smooth') + 
                          geom_abline(slope = 1, intercept = 0)
                    
            .. tab-item:: MatLab

            .. tab-item:: Python

    .. dropdown:: Normality of Error

        .. tab-set::

            .. tab-item:: Plain English

                We need to ensure that prediction errors are normally distributed. 
                If prediction errors are skewed, then our model is making making more underpredictions (negative) or overpredictions (positive) of :bdg-danger:`Decisions`.
                If the distribution is kurtosed (i.e. too skinny or too fat) then we're more or less likely to have extreme errors in predicting :bdg-danger:`Decisons` compared to a normal distribution. 
                We can create a density plot of prediction errors (i.e. the difference between model predictions of :bdg-danger:`Decisions` and observed :bdg-danger:`Decisions`) and see if it follows a bell-curve.
                We can create a density plot with a normal distribution where the standard deviation is the standard deviation of prediction errors to check this.

            .. tab-item:: R

                ::
                    
                    normvals = rnorm(1000, mean = 0, sd = sd(trialData$Prediction - trialData$Decision))
                    qplot(x = trialData$Prediction - trialData$Decision, geom = 'density', bw = 1, color = 'Actual') + 
                        geom_density(aes(x = normvals, color = 'Predicted'), bw = 1)
                    
            .. tab-item:: MatLab

            .. tab-item:: Python


    .. dropdown:: Independence of Error

        .. tab-set::

            .. tab-item:: Plain English

                Next, we need to ensure that our model's prediction errors of :bdg-danger:`Decisions` are not confounded with values of :bdg-primary:`Independent Variable`. 
                If our model predicts :bdg-danger:`Decisions` worse or better at certain values of :bdg-primary:`Independent Variable` compared to others, then our recovery of :bdg-success:`Free Parameters` is influenced disproportionately by :bdg-danger:`Decisions` made when :bdg-primary:`Independent Variable` have a certain value. 
                Thus, our :bdg-success:`Free Parameters` would be overfit for certain values and underfit for other values. 
                We can check this assumption by creating loess line with our :bdg-primary:`Independent Variable` on the x-axis and the model prediction errors on the y-axis.
                This slope should be 0 with an intercept of 0.

            .. tab-item:: R

                ::
                    
                    qplot(x = trialData$IV, y = (trialData$Prediction-trialData$Decision), geom = 'smooth')
                    
            .. tab-item:: MatLab

            .. tab-item:: Python

    .. dropdown:: Homoscedasticty

        .. tab-set::

            .. tab-item:: Plain English

                Finally, we need to ensure that variance in model prediction errors does not change as a function of an :bdg-primary:`Independent Variable`. 
                Of all of the assumptions, this one is the least problematic if violated - essentially it indicates that our model has less predictive accuracy at certain values of the :bdg-primary:`Independent Variable`. 
                This is a bigger issue for regression models because it can make Confidence Intervals of regression coefficients which are too narrow - we don't have this issue unless you're doing means testing on your :bdg-success:`Free Parameters`. 
                Nonetheless, this could produce unreliable :bdg-success:`Free Parameter` estimates so we have to rely on out-of-sample validation to rule this out.
                If this assumption is very badly violated, it might make sense to included a noise parameter into your model to scale with the :bdg-primary:`Independent Variable` or to, instead, use a different estimator such as Weighted Least Squares to recover your :bdg-success:`Free Parameters`.
                To check this, we can create a loess line with a variance cloud with our :bdg-primary:`Independent Variable` on the x-axis and the model prediction errors on the y-axis.
                The cloud should be a constant width around the loess line. 

            .. tab-item:: R

                ::
                    
                    qplot(x = trialData$IV, y = (trialData$Prediction-trialData$Decision), geom = 'smooth')
                    
            .. tab-item:: MatLab

            .. tab-item:: Python

.. dropdown:: Assessing Independence of Observations

    We want to ensure that accounting for :bdg-success:`Subjects`' differences from each other using :bdg-success:`Free Parameters` results in model predictions of :bdg-danger:`Decisions` which are not attributable to individual differences.
    We can accomplish this using the same linear modeling formula, but including random intercepts for the subject. 

    .. tab-set::

        .. tab-item:: Plain English

            First, we can try a model with a random intercept and random slope of model predicted value. 
            This model might have convergence issues or singularity issues - if so, great! Otherwise, don't worry just go to the next model. 

            Next, we can try a model with a only a random intercept. 
            We can estimate an R-squared for this model - the marginal R-squared (i.e. including variance explained by random effects) should be roughly equivalent to the conditional R-squared (i.e. variance only explained by fixed effects - i.e. your model) - within 0.05 is reasonable.

        .. tab-item:: R

            ::
                
                ris_model = lmer(data = trialData, Decision ~ Prediction + (1 + Prediction | SubjectID)) 
                summary(ris_model) #model should have issues
                ri_model = lmer(data = trialData, Decision ~ Prediction + (1 | SubjectID))
                summary(ri_model)
                library(MuMin)
                r.squaredGLMM(ri_model) #if conditional Rsq is between 0 and 0.05 lower than the multiple Rsq, that's good enough

In order to show that our parameter recovery process is robust, we have to show that the model can predict behavior that it was not trained on. 
We accomplish this by performing Fivefold Validation.

.. dropdown:: Fivefold Validation

    .. tab-set::

        .. tab-item:: Plain English

            We essentially want to prove that our :bdg-success:`Free Parameters` are not overfitting the :bdg-danger:`Decisions` that they are training on. 
            In other words, we want to rule out that our favored model isn't outperforming other models because it's fitting weird quirks in :bdg-success:`Subjects`' :bdg-danger:`Decisions`. 
            It should seem really intuitive that, in order to prove this, we're going to separate :bdg-danger:`Decisions` into a training set and a testing set. 
            Let's talk about how specifically we're going to do this. 

            So, we're going to randomly split each :bdg-success:`Subjects`' :bdg-danger:`Decisions` into one of five groups, called folds. 
            We're going to take one fold and remove those :bdg-primary:`Trials` from our training set - we're going to recover :bdg-success:`Free Parameters` from four-fifths of :bdg-danger:`Decisions` and we're going to test it on this fold - this fifth of :bdg-danger:`Decisions` that we excluded. 
            We're going to save the :bdg-success:`Free Parameters` from this four-fifths and the predicted :bdg-danger:`Decisions` of the withheld one-fifth. 
            We rinse and repeat for the remaining four folds. 
            Once this is done, we can get the model error for the predicted-against-observed :bdg-danger:`Decisions` for all :bdg-primary:`Trials`.
            Now we should have five sets of :bdg-success:`Free Parameters` for each :bdg-success:`Subject` and, combining the model error of all five folds, we should have a model error estimate across all :bdg-primary:`Trials`.

            Now, we want to look at two things: first we want to assess how much larger this model error is compared to the model error of the model trained on all of the data. 
            Here, you should report the change in root mean squared error per trial - you can also do a paired t-test on the MFI of the fivefold predictions compared to the standard model predictions but this is not necessary.
            Second, we also want to assess the similarity of the :bdg-success:`Free Parameters` we recovered withholding each of the five folds to the :bdg-success:`Free Parameters` recovered on the entire data set. 
            You should report the average cosine similarity across all folds for each of the :bdg-success:`Free Parameters`.

        .. tab-item:: R

            ::
                
                # define objective function
                obj_function_ff = function(params, decisions, wh){
                    Parameter1 = params[1]
                    Parameter2 = params[2]
                    
                    df = # a dataframe with your independent variables and constants
                    Choices = #
                    decisions = decisions[-wh]                    
                    predicted_u = vector('numeric', length(decisions))
                    observed_u = predicted_u
                    
                    for (j in 1:length(df$IV)){
                        u = vector('numeric', length(redistribution_rate))
                        for (k in 1:length(Choices)){
                            u[k] = Utility(Parameter1, Parameter2,
                                           construct1(df$IV[j], df$Constant[j], Choices[n]),
                                           construct2(df$IV[j], df$Constant[j], Choices[n]),
                                           construct3(df$IV[j], df$Constant[j], Choices[n]))
                        }
                        predicted_u[j] = max(u); observed_u[j] = u[(rr[j] * 10)+1]
                    }
                    return(sum(((predicted_u-observed_u))**2))
                }

                fivefold = data.frame() #preallocate for parameters and errors from the fivefold validation to go into

                for (i in 1:length(included_subjects)){
                    datafile = paste(parentfolder, included_subjects[i], restoffilepath, sep = '') # produces a character vector 'parentfolder/included_subjects[i]**.filetype'
                    df = read.csv2(datafile)
                    reorder = df$trialsTask.thisIndex + 1

                    order = sample(20)
                    Parameter1_ff = vector('numeric', length = 5)
                    Parameter2_ff = vector('numeric', length = 5)
                    SS_ff = 0
                    Prediction_ff = vector('numeric', length(df$Decision))
                    for (z in 1:5){
                        j = (z - 1) * 4 + 1
                        n = z * 4
                        withheld = order[j:n]
                        m = ((i - 1) * 5) + z
                        
                        result_ff = fmincon(obj_function_ff,x0 = initial_params, A = NULL, b = NULL, Aeq = NULL, beq = NULL,
                                            lb = lower_bounds, ub = upper_bounds,
                                            decisions = df$Decision[reorder], wh = withheld)
                        
                        Parameter1_ff[m] = result_ff$par[1]
                        Parameter2_ff[m] = result_ff$par[2]
                        for (n in 1:length(withheld)){
                            utility = vector('numeric', length(Choices))
                            for (q in 1:length(Choices)){
                                utility[q] = Utility(result_ff$par[1], result_ff$par[2],
                                                    construct1(df$IV[withheld[n]], df$Constant[withheld[n]], Choices[q]),
                                                    construct2(df$IV[withheld[n]], df$Constant[withheld[n]], Choices[q]),
                                                    construct3(df$IV[withheld[n]], df$Constant[withheld[n]], Choices[q]))
                            }
                            Prediction_ff[withheld[n]] = Choices[which(utility == max(utility))]
                        }
                    }
                    SS_ff = sum((df$Decision - Prediction_ff)**2)
                    fivefold[i, 1:11] = c(SS_ff, Parameter1_ff, Parameter2_ff)
                }
                colnames(fivefold) = c('SS', 'Par1_fold1', 'Par1_fold2', 'Par1_fold3', 'Par1_fold4', 'Par1_fold5', 
                                       'Par2_fold1', 'Par2_fold2', 'Par2_fold3', 'Par2_fold4', 'Par2_fold5', SubjectID)

                sqrt(mean(fivefold$SS)/length(df$IV)) - sqrt(mean(subjectData$modelSS)/length(df$IV)) #the change in root mean squared error, per trial
                fivefold$AIC = length(df$IV) * log(fivefold$SS/length(df$IV)) + 2 * 2
                t.test(fivefold$AIC, subjectData$AIC, paired = T) #test fivefold MFI against normal MFI for this model

                library(lsa)
                cosines = vector('numeric', length = 10)
                for (i in 1:5){
                    cosines[i] = cosine(subjectData$Parameter1, fivefold[, (i + 1)]) #to get the correct columns in the fivefold dataframe (2-6)
                    cosines[(i+5)] = cosine(subjectData$Parameter1, fivefold[, (i + 6)]) #to get the correct columns in the fivefold dataframe (7-11)
                }

                mean(cosines[1:5]) #cosine similarity of parameter 1
                mean(cosines[6:10]) #cosine similarity of parameter 2


Tutorial
==========