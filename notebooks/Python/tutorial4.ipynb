{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Li et al., 2022\n",
    "\n",
    "## 1.4 Model the Data Generation Process\n",
    "\n",
    "First, let's define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def inequality(a1, b1, a2, b2):\n",
    "    d_Inequality = abs(a2 - b2) - abs(a1 - b1)\n",
    "    return d_Inequality\n",
    "\n",
    "def harm(a0, b0, a1, b1, a2, b2):\n",
    "    initial = np.array([a0, b0])\n",
    "    choice1 = np.array([a1, b1])\n",
    "    choice2 = np.array([a2, b2])\n",
    "    advantaged = 1 if a0 == b0 else np.argmax(initial)\n",
    "    d_lossAdvantaged = (initial[advantaged] - choice1[advantaged]) - (initial[advantaged] - choice2[advantaged])\n",
    "    return d_lossAdvantaged\n",
    "\n",
    "def rankReverse(a0, b0, a1, b1, a2, b2):\n",
    "    if a0 == b0:\n",
    "        return 0\n",
    "    d_initial = a0 - b0\n",
    "    d_choice1 = a1 - b1\n",
    "    d_choice2 = a2 - b2\n",
    "    if d_initial > 0:\n",
    "        choice1Reversed = 1 if d_choice1 < 0 else 0\n",
    "        choice2Reversed = 1 if d_choice2 < 0 else 0\n",
    "    else:\n",
    "        choice1Reversed = 1 if d_choice1 > 0 else 0\n",
    "        choice2Reversed = 1 if d_choice2 > 0 else 0\n",
    "    return choice1Reversed - choice2Reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check and see if they do what we want. Let's make an example trial for two players A and B with starting allocations a0 and b0 and possible choices 1 and 2 with different distributions between player A and player B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame({'a0': [20], 'b0': [0], 'a1': [8], 'b1': [12], 'a2': [15], 'b2': [5]})\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view the outputs of this example to make sure they look correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inequality(example['a1'][0], example['b1'][0], example['a2'][0], example['b2'][0]))\n",
    "print(harm(example['a0'][0], example['b0'][0], example['a1'][0], example['b1'][0], example['a2'][0], example['b2'][0]))\n",
    "print(rankReverse(example['a0'][0], example['b0'][0], example['a1'][0], example['b1'][0], example['a2'][0], example['b2'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So choice 1 is 6 more equal, causes more 7 harm, and reverses the existing rank. This is correct!\n",
    "\n",
    "## 1.5 Simulating Data\n",
    "\n",
    "Now let's preallocate and define functions, triallist, and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialList = pd.DataFrame({\n",
    "    'a0': np.random.randint(10, 21, 100)\n",
    "})\n",
    "trialList['b0'] = 20 - trialList['a0']\n",
    "trialList['a1'] = [np.random.randint(5, a0 + 1) for a0 in trialList['a0']]\n",
    "trialList['b1'] = 20 - trialList['a1']\n",
    "trialList['a2'] = [np.random.randint(5, a0 + 1) for a0 in trialList['a0']]\n",
    "trialList.loc[trialList['a2'] == trialList['a1'], ('a2')] = 10\n",
    "trialList['b2'] = 20 - trialList['a2']\n",
    "trialList = pd.concat([trialList, trialList[['b0', 'a0', 'b1', 'a1', 'b2', 'a2']]], ignore_index=True)\n",
    "\n",
    "trialList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(pars, IVs):\n",
    "    IVS = np.array(IVs)\n",
    "    a0, b0, a1, b1, a2, b2 = IVs[:6]\n",
    "    alpha, delta, rho = pars[:3]\n",
    "    ineq = inequality(a1, b1, a2, b2)\n",
    "    hrm = harm(a0, b0, a1, b1, a2, b2)\n",
    "    rank = rankReverse(a0, b0, a1, b1, a2, b2)\n",
    "    return (alpha * ineq) - (delta * hrm) - (rho * rank)\n",
    "\n",
    "def probability(pars, utilitydiff):\n",
    "    beta, epsilon, gamma = pars[-3:]\n",
    "    prob = 1 / (1 + np.exp(-(beta * utilitydiff)))\n",
    "    prob = prob * (1 - 2 * epsilon) + epsilon + gamma * (2 * epsilon)\n",
    "    return prob\n",
    "\n",
    "freeParameters = pd.DataFrame({\n",
    "    'alpha': np.tile(np.arange(0, 2, 0.1), 60) + np.random.choice(np.arange(0, 0.1, 0.001), 20 * 6 * 10),\n",
    "    'delta': np.tile(np.arange(0, 2, 0.1), 60) + np.random.choice(np.arange(0, 0.1, 0.001), 20 * 6 * 10),\n",
    "    'rho': np.tile(np.arange(0, 2, 0.1), 60) + np.random.choice(np.arange(0, 0.1, 0.001), 20 * 6 * 10),\n",
    "    'beta': np.random.choice(np.arange(0, 11), 20 * 6 * 10),\n",
    "    'epsilon': np.repeat(np.repeat(np.arange(0, 0.6, 0.1), 10), 20),\n",
    "    'gamma': np.repeat(np.arange(-0.5, 0.5, 0.1), 20 * 6) + np.random.choice(np.arange(0, 0.1, 0.001), 20 * 6 * 10)\n",
    "})\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "def generatePredictions(parameters, df):\n",
    "    pred = np.zeros(len(df))\n",
    "    for i in range(len(df)):\n",
    "        thisTrialIVs = df.iloc[i].to_numpy()\n",
    "        utilityDiff = utility(parameters, thisTrialIVs)\n",
    "        pred[i] = max(min(probability(parameters, utilityDiff), 0.9999999999), 0.00000000001)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of that's done, let's generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(freeParameters)):\n",
    "    pars = freeParameters.iloc[i]\n",
    "    predictions.loc[i, range(0, len(trialList))] = generatePredictions(pars, trialList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Compare Recovered Parameters\n",
    "\n",
    "Let's write the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_function(params, df, optimMethod=\"MLE\"):\n",
    "    Prob1 = generatePredictions(params, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1) ** 2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimize(obj, initial_params, lower_bounds, upper_bounds, df):\n",
    "    try:\n",
    "        result = minimize(obj, initial_params, args=(df,), bounds=list(zip(lower_bounds, upper_bounds)), tol=1e-08)\n",
    "    except:\n",
    "        result = minimize(obj, initial_params, args=(df,), bounds=list(zip(lower_bounds, upper_bounds)), tol=1e-08, method=\"L-BFGS-B\")\n",
    "    return result\n",
    "\n",
    "freeParameters[['alphaRecovered', 'deltaRecovered', 'rhoRecovered', 'betaRecovered', 'epsilonRecovered', 'gammaRecovered']] = 0.0\n",
    "\n",
    "initial_params = [1, 1, 1, 4, 0.25, 0]\n",
    "lower_bounds = [0, 0, 0, 0, 0, -0.5]\n",
    "upper_bounds = [2, 2, 2, 10, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can recover the free parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(freeParameters)):\n",
    "    trialList['Predictions'] = predictions.loc[i].to_numpy()\n",
    "    result = optimize(obj_function, initial_params, lower_bounds, upper_bounds, trialList)\n",
    "    freeParameters.loc[i, ['alphaRecovered', 'deltaRecovered', 'rhoRecovered', 'betaRecovered', 'epsilonRecovered', 'gammaRecovered']] = result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now assess the reliability of our parameter recovery process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "freeParameters['Epsilon'] = freeParameters['epsilon'].astype('category')\n",
    "\n",
    "sns.lmplot(data=freeParameters, x='alpha', y='alphaRecovered', hue='Epsilon', lowess=True, ci=None, scatter_kws={'s': 50})\n",
    "plt.plot([freeParameters['alpha'].min(), freeParameters['alpha'].max()],\n",
    "         [freeParameters['alphaRecovered'].min(), freeParameters['alphaRecovered'].max()], 'k--')\n",
    "plt.show()\n",
    "\n",
    "sns.lmplot(data=freeParameters, x='delta', y='deltaRecovered', hue='Epsilon', lowess=True, ci=None, scatter_kws={'s': 50})\n",
    "plt.plot([freeParameters['delta'].min(), freeParameters['delta'].max()],\n",
    "         [freeParameters['deltaRecovered'].min(), freeParameters['deltaRecovered'].max()], 'k--')\n",
    "plt.show()\n",
    "\n",
    "sns.lmplot(data=freeParameters, x='rho', y='rhoRecovered', hue='Epsilon', lowess=True, ci=None, scatter_kws={'s': 50})\n",
    "plt.plot([freeParameters['rho'].min(), freeParameters['rho'].max()],\n",
    "         [freeParameters['rhoRecovered'].min(), freeParameters['rhoRecovered'].max()], 'k--')\n",
    "plt.show()\n",
    "\n",
    "sns.lmplot(data=freeParameters, x='beta', y='betaRecovered', hue='Epsilon', lowess=True, ci=None, scatter_kws={'s': 50})\n",
    "plt.plot([freeParameters['beta'].min(), freeParameters['beta'].max()],\n",
    "         [freeParameters['betaRecovered'].min(), freeParameters['betaRecovered'].max()], 'k--')\n",
    "plt.show()\n",
    "\n",
    "sns.lmplot(data=freeParameters, x='epsilon', y='epsilonRecovered', lowess=True, ci=None, scatter_kws={'s': 50})\n",
    "plt.plot([freeParameters['epsilon'].min(), freeParameters['epsilon'].max()],\n",
    "         [freeParameters['epsilonRecovered'].min(), freeParameters['epsilonRecovered'].max()], 'k--')\n",
    "plt.show()\n",
    "\n",
    "sns.lmplot(data=freeParameters, x='gamma', y='gammaRecovered', hue='Epsilon', lowess=True, ci=None, scatter_kws={'s': 50})\n",
    "plt.plot([freeParameters['gamma'].min(), freeParameters['gamma'].max()],\n",
    "         [freeParameters['gammaRecovered'].min(), freeParameters['gammaRecovered'].max()], 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these look particularly reliable (even at low values of epsilon). The exception is epsilon.\n",
    "\n",
    "## 2.1 Recovering Free Parameters\n",
    "\n",
    "First let's get the trial data from participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialData = pd.read_csv(\"C:/Users/DELL/Downloads/Data/Data/HPP_fMRI_beh_data_for_lmm.csv\", sep=',')\n",
    "trialData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need participants' ID numbers, a0-b2, and their choices in the format 1 or 2. In the above data frame, those are columns 1, 6, 7, 13, 14, 21, 22, and 26 so let's extract those. Also, they only analyzed the rank-reverse condition (perhaps this is why we couldn't reliably recover parameters) so that is \"trail_type\" 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialData = trialData.iloc[np.where(trialData['trail_type'] == 3)[0].tolist(), :]\n",
    "trialData = trialData.iloc[:, [0,5,6,12,13,20,21,35]]\n",
    "trialData.columns = ['SubjectID', 'a0', 'b0', 'a1', 'b1', 'a2', 'b2', 'Chose1']\n",
    "trialData['Chose1'] -= 1\n",
    "trialData['Prob1'] = 0.0\n",
    "trialData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define some necessary variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_subjects = trialData['SubjectID'].unique()\n",
    "subjectData = pd.DataFrame()\n",
    "\n",
    "def grab_data(subject):\n",
    "    return trialData[trialData['SubjectID'] == subject].drop(columns=['SubjectID'])\n",
    "\n",
    "def addPredictions(trialData, subject, predictions):\n",
    "    trialData.loc[trialData['SubjectID'] == subject, 'Prob1'] = predictions\n",
    "    return trialData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which allows us to Recover Free Parameters and Define Predicted Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(included_subjects)):\n",
    "    df = grab_data(included_subjects[i])\n",
    "    result = optimize(obj_function, initial_params, lower_bounds, upper_bounds, df)\n",
    "    df['Prob1'] = generatePredictions(result.x, df)\n",
    "    model_SS = np.sum((df['Chose1'] - df['Prob1']) ** 2)\n",
    "    model_NLL = -2 * np.sum(df['Chose1'] * np.log(df['Prob1']) + (1 - df['Chose1']) * np.log(1 - df['Prob1']))\n",
    "    subjectData.loc[i, range(0, 9)] = [included_subjects[i]] + result.x.tolist() + [model_SS, model_NLL]\n",
    "    trialData = addPredictions(trialData, included_subjects[i], df['Prob1'])\n",
    "\n",
    "subjectData.columns = [\"subjectID\", \"Alpha\", \"Delta\", \"Rho\", \"Beta\", \"Epsilon\", \"Gamma\", \"SS\", \"Deviance\"]\n",
    "subjectData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compute Model Fit Index\n",
    "\n",
    "We will calculate BIC as the model fit index because we are attempting to model the probabilistic nature of the data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectData['BIC'] = subjectData['Deviance'] + np.log(65) * 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Identity the Best Model\n",
    "\n",
    "We need to define new objective functions for each model. Since each model uses the same utility function, but holds some variables constant (at 0), we only really need to modify the number of parameter inputs and set the constant values to 0. We can also use a list of indices so that the we can use the same function to generate predictions down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def of_alphaOnly(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[0, 3, 4, 5]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_deltaOnly(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[1, 3, 4, 5]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_rhoOnly(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[2, 3, 4, 5]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_ad(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[0, 1, 3, 4, 5]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_ar(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[0, 2, 3, 4, 5]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_dr(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[1, 2, 3, 4, 5]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_noEpsilon(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[0, 1, 2, 3]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_noGamma(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[[0, 1, 2, 3, 4]] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "def of_GammaOnly(params, df, optimMethod=\"MLE\"):\n",
    "    params_new = np.zeros(6)\n",
    "    params_new[4] = 0.5\n",
    "    params_new[5] = params\n",
    "    Prob1 = generatePredictions(params_new, df)\n",
    "    Chose1 = df.iloc[:, 6]\n",
    "    if optimMethod == \"OLS\":\n",
    "        return np.sum((Chose1 - Prob1)**2)\n",
    "    elif optimMethod == \"MLE\":\n",
    "        return -np.sum(Chose1 * np.log(Prob1) + (1 - Chose1) * np.log(1 - Prob1))\n",
    "\n",
    "ofs = [of_alphaOnly, of_deltaOnly, of_rhoOnly, of_ad, of_ar, of_dr, of_noEpsilon, of_noGamma, of_GammaOnly]\n",
    "idxs = [[0, 3, 4, 5], [1, 3, 4, 5], [2, 3, 4, 5], [0, 1, 3, 4, 5], [0, 2, 3, 4, 5], [1, 2, 3, 4, 5], [0, 1, 2, 3], [0, 1, 2, 3, 4], [5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can preallocate the predictions for each model and the new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "altSubjectData = pd.DataFrame()\n",
    "altTrialData = trialData.drop(columns=[trialData.columns[8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's Recover Free Parameters and Generate Predictions for this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(included_subjects)):\n",
    "    df = grab_data(included_subjects[i])\n",
    "    outputs = []\n",
    "    j = 0\n",
    "    \n",
    "    for k in range(0, len(idxs)):\n",
    "        idx = idxs[k]\n",
    "        initials = [initial_params[i] for i in idx]\n",
    "        uppers = [upper_bounds[i] for i in idx]\n",
    "        lowers = [lower_bounds[i] for i in idx]\n",
    "        of = ofs[k]\n",
    "        \n",
    "        result = optimize(of, initials, lowers, uppers, df)\n",
    "\n",
    "        pars = np.zeros(6)\n",
    "        pars[idx] = result.x\n",
    "        if len(idx) == 1:\n",
    "            pars[4] = 0.5\n",
    "        \n",
    "        df['Prob1'] = generatePredictions(pars, df)\n",
    "        \n",
    "        model_SS = np.sum((df['Chose1'] - df['Prob1'])**2)\n",
    "        model_NLL = -2 * np.sum(df['Chose1'] * np.log(df['Prob1']) + (1 - df['Chose1']) * np.log(1 - df['Prob1']))\n",
    "        outputs = outputs + result.x.tolist() + [model_SS, model_NLL]\n",
    "        j += 2 + len(result.x)\n",
    "        \n",
    "        altTrialData.loc[altTrialData['SubjectID'] == included_subjects[i], 8 + k] = df['Prob1']\n",
    "    \n",
    "    altSubjectData.loc[i, range(0, 56)] = [included_subjects[i]] + outputs\n",
    "\n",
    "altSubjectData.columns = [\n",
    "    'SubjectID', \n",
    "    'Alpha_M1', 'Beta_M1', 'Epsilon_M1', 'Gamma_M1', 'SS_M1', 'Deviance_M1',\n",
    "    'Delta_M2', 'Beta_M2', 'Epsilon_M2', 'Gamma_M2', 'SS_M2', 'Deviance_M2', \n",
    "    'Rho_M3', 'Beta_M3', 'Epsilon_M3', 'Gamma_M3', 'SS_M3', 'Deviance_M3', \n",
    "    'Alpha_M4', 'Delta_M4', 'Beta_M4', 'Epsilon_M4', 'Gamma_M4', 'SS_M4', 'Deviance_M4',\n",
    "    'Alpha_M5', 'Rho_M5', 'Beta_M5', 'Epsilon_M5', 'Gamma_M5', 'SS_M5', 'Deviance_M5',\n",
    "    'Delta_M6', 'Rho_M6', 'Beta_M6', 'Epsilon_M6', 'Gamma_M6', 'SS_M6', 'Deviance_M6',\n",
    "    'Alpha_M7', 'Delta_M7', 'Rho_M7', 'Beta_M7', 'SS_M7', 'Deviance_M7',\n",
    "    'Alpha_M8', 'Delta_M8', 'Rho_M8', 'Beta_M8', 'Epsilon_M8', 'SS_M8', 'Deviance_M8',\n",
    "    'Gamma_M9', 'SS_M9', 'Deviance_M9'\n",
    "]\n",
    "\n",
    "altTrialData.columns = ['SubjectID', 'a0', 'b0', 'a1', 'b1', 'a2', 'b2', 'Chose1','alphaOnly_Prob1', 'deltaOnly_Prob1', 'rhoOnly_Prob1', 'ad_Prob1', 'ar_Prob1', 'dr_Prob1', 'noEpsilon_Prob1', 'noGamma_Prob1', 'gammaOnly_Prob1']\n",
    "\n",
    "for col in altSubjectData.columns[1:]:\n",
    "    altSubjectData[col] = pd.to_numeric(altSubjectData[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's glance at the trial level data for these alternative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altTrialData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute BIC for these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "altSubjectData['BIC_M1'] = altSubjectData['Deviance_M1'] + np.log(65) * 4\n",
    "altSubjectData['BIC_M2'] = altSubjectData['Deviance_M2'] + np.log(65) * 4\n",
    "altSubjectData['BIC_M3'] = altSubjectData['Deviance_M3'] + np.log(65) * 4\n",
    "altSubjectData['BIC_M4'] = altSubjectData['Deviance_M4'] + np.log(65) * 5\n",
    "altSubjectData['BIC_M5'] = altSubjectData['Deviance_M5'] + np.log(65) * 5\n",
    "altSubjectData['BIC_M6'] = altSubjectData['Deviance_M6'] + np.log(65) * 5\n",
    "altSubjectData['BIC_M7'] = altSubjectData['Deviance_M7'] + np.log(65) * 4\n",
    "altSubjectData['BIC_M8'] = altSubjectData['Deviance_M8'] + np.log(65) * 5\n",
    "altSubjectData['BIC_M9'] = altSubjectData['Deviance_M9'] + np.log(65) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compare the BIC of all of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBIC = [subjectData['BIC'].sum(), altSubjectData['BIC_M1'].sum(), altSubjectData['BIC_M2'].sum(), altSubjectData['BIC_M3'].sum(), altSubjectData['BIC_M4'].sum(), altSubjectData['BIC_M5'].sum(), altSubjectData['BIC_M6'].sum(), altSubjectData['BIC_M7'].sum(), altSubjectData['BIC_M8'].sum(), altSubjectData['BIC_M9'].sum()]\n",
    "print(np.argmin(modelBIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis indicates that the best model has Alpha, Delta, Beta, Epsilon, and Gamma - not Rho (i.e. Rank Reversal Aversion).\n",
    "\n",
    "## Validate the Best Model\n",
    "\n",
    "First, let's assess model performance at a basic level: we can look at prediction accuracy to begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(altTrialData['Chose1'] == np.round(altTrialData['ad_Prob1'])) / len(altTrialData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can proceed to look at the distribution of model accuracy across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altTrialData['a0-a1'] = altTrialData['a0']-altTrialData['a1']\n",
    "altTrialData['a0-a2'] = altTrialData['a0']-altTrialData['a2']\n",
    "altTrialData['b0-b1'] = altTrialData['b0']-altTrialData['b1']\n",
    "altTrialData['b0-b2'] = altTrialData['b0']-altTrialData['b2']\n",
    "altTrialData['a1-a2'] = altTrialData['a1']-altTrialData['a2']\n",
    "altTrialData['b1-b2'] = altTrialData['b1']-altTrialData['b2']\n",
    "altTrialData['a0_less_than_b0'] = trialData['a0'] < trialData['b0']\n",
    "altTrialData['Chose1 - Prob1'] = altTrialData['Chose1']-altTrialData['ad_Prob1']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(altSubjectData['BIC_M4'])\n",
    "plt.title('Density Plot of BIC Values')\n",
    "plt.xlabel('BIC')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the bottom 25% of model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_summary = altSubjectData['BIC_M4'].describe()\n",
    "worst_explained = altSubjectData.index[altSubjectData['BIC_M4'] > bic_summary['75%']].tolist()\n",
    "worst_subject_ids = altSubjectData['SubjectID'].iloc[worst_explained]\n",
    "filtered_trialData = altTrialData[altTrialData['SubjectID'].isin(worst_subject_ids)]\n",
    "\n",
    "sns.lmplot(data=filtered_trialData, x='a0-a1', y='Chose1 - Prob1', hue='SubjectID', legend=False, lowess=True)\n",
    "plt.title('a0 - a1 vs Chose1 - Prob1')\n",
    "plt.xlabel('a0 - a1')\n",
    "plt.ylabel('Chose1 - Prob1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks systematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=filtered_trialData, x='a0-a2', y='Chose1 - Prob1', hue='SubjectID', legend=False, lowess=True)\n",
    "plt.title('a0 - a2 vs Chose1 - Prob1')\n",
    "plt.xlabel('a0 - a2')\n",
    "plt.ylabel('Chose1 - Prob1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as much here, but still some directionality within people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=filtered_trialData, x='b0-b1', y='Chose1 - Prob1', hue='SubjectID', legend=False, lowess=True)\n",
    "plt.title('b0 - b1 vs Chose1 - Prob1')\n",
    "plt.xlabel('b0 - b1')\n",
    "plt.ylabel('Chose1 - Prob1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same here, not as bad as the first but still not so great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=filtered_trialData, x='b0-b2', y='Chose1 - Prob1', hue='SubjectID', legend=False, lowess=True)\n",
    "plt.title('b0 - b2 vs Chose1 - Prob1')\n",
    "plt.xlabel('b0 - b2')\n",
    "plt.ylabel('Chose1 - Prob1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the inverse of the first graph. This is likely an artifact of the lack of accounting for rank reversal aversion. Keeping in mind that we are dealing with the worst explained individuals, this makes sense and it's not the end of the world (assuming that the model's assumptions are not severely violated across the whole sample). There's nothing that we can really do about the fact that we have this result, other than to bear it in mind as we view our results because we have committed to testing this model and have found that it explains the data better. But we can entertain the full model for exploratory purposes potentially.\n",
    "\n",
    "And nothing here either. Now we can check assumptions: first linearity (we'll do this across both choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=altTrialData, x='ad_Prob1', y='Chose1', hue='a0_less_than_b0', lowess = True)\n",
    "plt.title('Smooth Plot of Prob1 vs Chose1')\n",
    "plt.xlabel('Prob1')\n",
    "plt.ylabel('Chose1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that something about a0 and b0 is different: our model seems unable to account for this bias so that's slightly alarming. Looks okay, not exactly perfect but still decent. Second, normality of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = altTrialData['ad_Prob1'] - trialData['Chose1']\n",
    "altTrialData['residuals'] = residuals\n",
    "normvals = np.random.normal(loc=0, scale=np.std(residuals), size=1000)\n",
    "\n",
    "sns.kdeplot(residuals, bw_adjust=np.std(normvals)*10, label='Actual', color='blue')\n",
    "sns.kdeplot(normvals, bw_adjust=np.std(normvals)*10, label='Predicted', color='red')\n",
    "plt.title('Density Plot of Residuals vs Normal Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one, I played with the factor that I multiplied the standard deviation of the normvals by in assessing the bw_adjust argument. You can see that if you set it to 1 (i.e. the standard deviation), the distributions look very choppy. Here, we want to use the theoretical (i.e. normvals) standard deviation to avoid any bias in the empirical data. We see that the distribution looks mostly normal, not visually skewed and somewhat lepokurtic - not super problematic though. Third we can examine homoscedasticity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=altTrialData, x='a1-a2', y='residuals', scatter=False)\n",
    "plt.title('Abs(a1 - a2) vs Residuals')\n",
    "plt.xlabel('Abs(a1 - a2)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice constant variance cloud, slightly positive but looking at the X axis, we can see that the discrepency is about 6% over all ranges of values so it's not cataclysmic. And finally independence of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=altTrialData, x='a1-a2', y='residuals', hue='a0_less_than_b0', palette='coolwarm', lowess=True)\n",
    "plt.title('Abs(a1 - a2) vs Residuals Colored by a0 > b0 with Loess Smoothing')\n",
    "plt.xlabel('Abs(a1 - a2)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(title='a0 > b0', labels=['False', 'True'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not perfect but not horrible either.\n",
    "\n",
    "If you want to assess independence explicitly, you can run the analysis in R. Running lmer models in Python with random slopes requires (to the best of my knowledge) you to use packages that allow you to run R processes anyways so no need to overcomplicate things. This step is extra really, so you don't fully need to do it.\n",
    "\n",
    "Let's jump to fivefold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivefold = pd.DataFrame()\n",
    "trialData['Prob1_ff'] = 0.0\n",
    "initial_params = [1, 1, 4, 0.25, 0]\n",
    "lower_bounds = [0, 0, 0, 0, -0.5]\n",
    "upper_bounds = [2, 2, 10, 0.5, 0.5]\n",
    "\n",
    "for i in range(0, len(included_subjects)):\n",
    "    df = grab_data(included_subjects[i])\n",
    "    df = df.reset_index()\n",
    "    df['Prob1'] = 0.0\n",
    "    \n",
    "    order = np.random.permutation(len(df))\n",
    "    \n",
    "    A_ff, D_ff, B_ff, E_ff, G_ff = np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5), np.zeros(5)\n",
    "    Deviance_ff = 0.0\n",
    "    \n",
    "    for z in range(5):\n",
    "        j = int((z) * (len(df) / 5))\n",
    "        n = int((z+1) * (len(df) / 5))\n",
    "        withheld = order[j:n]\n",
    "\n",
    "        input = df.iloc[~df.index.isin(withheld)]\n",
    "        input = input.loc[:, ['a0', 'b0', 'a1', 'b1', 'a2', 'b2', 'Chose1', 'Prob1']]\n",
    "        \n",
    "        result_ff = optimize(of_ad, initial_params, lower_bounds, upper_bounds, input)\n",
    "        \n",
    "        A_ff[z], D_ff[z], B_ff[z], E_ff[z], G_ff[z] = result_ff.x\n",
    "        input = df.iloc[df.index.isin(withheld)]\n",
    "        input = input.loc[:, ['a0', 'b0', 'a1', 'b1', 'a2', 'b2', 'Chose1', 'Prob1']]\n",
    "        pars = result_ff.x[0:2].tolist() + [0] + result_ff.x[2:].tolist()\n",
    "        df.loc[df.index.isin(withheld),'Prob1'] = generatePredictions(pars, input)\n",
    "    \n",
    "    Deviance_ff = -2 * np.sum(df['Chose1'] * np.log(df['Prob1']) + (1 - df['Chose1']) * np.log(1 - df['Prob1']))\n",
    "    fivefold.loc[i, range(0, 27)] = [included_subjects[i], Deviance_ff] + list(A_ff) + list(D_ff) + list(B_ff) + list(E_ff) + list(G_ff)\n",
    "    trialData.loc[trialData['SubjectID'] == included_subjects[i], 'Prob1_ff'] = df['Prob1']\n",
    "\n",
    "fivefold.columns = ['SubjectID', 'Deviance', 'A_F1', 'A_F2', 'A_F3', 'A_F4', 'A_F5',\n",
    "                    'D_F1', 'D_F2', 'D_F3', 'D_F4', 'D_F5', 'B_F1', 'B_F2', 'B_F3', 'B_F4', 'B_F5', \n",
    "                    'E_F1', 'E_F2', 'E_F3', 'E_F4', 'E_F5', 'G_F1', 'G_F2', 'G_F3', 'G_F4', 'G_F5']\n",
    "fivefold['BIC'] = fivefold['Deviance'] + np.log(65) * 5\n",
    "fivefold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(round(trialData['Prob1_ff']) == trialData['Chose1'])/len(trialData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not such a big loss in accuracy. And let's test against the normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "print(ttest_rel(fivefold['BIC'], altSubjectData['BIC_M4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's significantly higher, but considering that we haven't lost so much in accuracy it should be fine. And to assess the reliability, we can compute cosine similiarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosines = []\n",
    "for i in range(5):\n",
    "    cosines.append(cosine_similarity([altSubjectData['Alpha_M4'].values], [fivefold.iloc[:, i + 2].values])[0][0])\n",
    "    cosines.append(cosine_similarity([altSubjectData['Delta_M4'].values], [fivefold.iloc[:, i + 7].values])[0][0])\n",
    "    cosines.append(cosine_similarity([altSubjectData['Beta_M4'].values], [fivefold.iloc[:, i + 10].values])[0][0])\n",
    "    cosines.append(cosine_similarity([altSubjectData['Epsilon_M4'].values], [fivefold.iloc[:, i + 17].values])[0][0])\n",
    "    cosines.append(cosine_similarity([altSubjectData['Gamma_M4'].values], [fivefold.iloc[:, i + 22].values])[0][0])\n",
    "\n",
    "cosines = np.array(cosines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can look at alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cosines[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good. Now we can look at delta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cosines[5:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also good. We can check beta now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cosines[10:15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Epsilon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cosines[15:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same. And finally Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cosines[20:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good.\n",
    "\n",
    "## 3.1 Compare Models\n",
    "\n",
    "Let's first see if the best model (inequality aversion and rank-reverse aversion, but not harm aversion) outperforms the model with all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], subjectData['BIC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. How about only alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], altSubjectData['BIC_M1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better but not significantly. How about delta only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], altSubjectData['BIC_M2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly better than delta only. Rho only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], altSubjectData['BIC_M3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, what about alpha and rho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], altSubjectData['BIC_M5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not significantly better. What about delta and rho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], altSubjectData['BIC_M6']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Okay all done here.\n",
    "\n",
    "## 3.3 Test for Individual Differences\n",
    "\n",
    "Let's first recover parameters over the whole dataset and we can assess how accurate it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultNID = optimize(of_ad, initial_params, lower_bounds, upper_bounds, trialData.iloc[:, 1:8])\n",
    "pars = resultNID.x[0:2].tolist() + [0] + resultNID.x[2:].tolist()\n",
    "trialData['Prob1_NID'] = generatePredictions(pars, trialData)\n",
    "print(np.sum(trialData['Chose1'] == np.round(trialData['Prob1_NID'])) / len(trialData))\n",
    "altSubjectData['Deviance_NID'] = 0.0\n",
    "for i, subject in enumerate(included_subjects):\n",
    "    trials = trialData['SubjectID'] == subject\n",
    "    df = trialData[trials]\n",
    "    altSubjectData.loc[i, 'Deviance_NID'] = -2 * np.sum(df['Chose1'] * np.log(df['Prob1_NID']) + (1 - df['Chose1']) * np.log(1 - df['Prob1_NID']))\n",
    "\n",
    "altSubjectData['BIC_NID'] = altSubjectData['Deviance_NID'] + np.log(65) * 5 / len(included_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good. This isn't surprising given that people often have very different preferences. Now let's test for individual differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttest_rel(altSubjectData['BIC_M4'], altSubjectData['BIC_NID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant individual differences. Let's see which models are worse than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(modelBIC > np.sum(altSubjectData['BIC_NID']))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the only Harm Aversion, only Rank Reversal Aversion, Harm and Rank Reversal Aversion, and only harm and rank reversal models are worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
